{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import operator\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from seqlbtoolkit.embs import build_bert_token_embeddings\n",
    "from seqlbtoolkit.text import split_overlength_bert_input_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('a.json', 'r', encoding='utf-8') as f:\n",
    "    text_seq = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encs = tokenizer(text_seq,\n",
    "                 is_split_into_words=True,\n",
    "                 add_special_tokens=True,\n",
    "                 return_offsets_mapping=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512\n",
    "tk_seq_list = [text_seq]\n",
    "sent_lengths_list = [None]\n",
    "\n",
    "split_tk_seq_list = list()\n",
    "ori2split_ids_map = list()\n",
    "n = 0\n",
    "\n",
    "# update input sentences so that every sentence has BERT length < 510\n",
    "for tk_seq, sent_lens in zip(tk_seq_list, sent_lengths_list):\n",
    "\n",
    "    if sent_lens:\n",
    "        ends = list(itertools.accumulate(sent_lens, operator.add))\n",
    "        starts = [0] + ends[:-1]\n",
    "        tk_seq_ = [tk_seq[s:e] for s, e in zip(starts, ends)]\n",
    "    else:\n",
    "        tk_seq_ = [tk_seq]\n",
    "\n",
    "    tk_seqs = split_overlength_bert_input_sequence(tk_seq_, tokenizer, max_seq_length)\n",
    "    n_splits = len(tk_seqs)\n",
    "    split_tk_seq_list += tk_seqs\n",
    "\n",
    "    ori2split_ids_map.append(list(range(n, n + n_splits)))\n",
    "    n += n_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_tk_seq_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tk_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tk_seqs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d00f8952959be9fb2f977b85902a6101fd652db9820ce2b143b005325f5b1ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
