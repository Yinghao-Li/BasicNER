{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import itertools\n",
    "import operator\n",
    "from lstm_crf.dataset import Dataset\n",
    "from lstm_crf.args import Config\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from seqlbtoolkit.text import split_overlength_bert_input_sequence\n",
    "from seqlbtoolkit.data import merge_list_of_lists\n",
    "from seqlbtoolkit.io import save_json\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('./data/MSIE/train.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 512\n",
    "\n",
    "for data_id in range(len(data)):\n",
    "    print(data_id)\n",
    "    tks = data[f'{data_id}']['data']['text']\n",
    "    tks_seq_list = [word_tokenize(sent) for sent in sent_tokenize(' '.join(tks))]\n",
    "    seq_bert_len_list = [len(tokenizer.tokenize(' '.join(tks_seq), add_special_tokens=True)) for tks_seq in tks_seq_list]\n",
    "    split_points = [0, len(tks_seq_list)]\n",
    "    split_bert_lens = [sum(seq_bert_len_list[split_points[i]:split_points[i+1]]) for i in range(len(split_points)-1)]\n",
    "\n",
    "    while (np.asarray(split_bert_lens) > max_seq_length).any():\n",
    "\n",
    "        new_split_points = list()\n",
    "        for idx, bert_len in enumerate(split_bert_lens):\n",
    "            # print(idx, bert_len)\n",
    "            if bert_len > max_seq_length:\n",
    "                seq_bert_len_sub_list = seq_bert_len_list[split_points[idx]:split_points[idx+1]]\n",
    "                seq_bert_len_sub_accu_list = list(itertools.accumulate(seq_bert_len_sub_list, operator.add))\n",
    "                # print(f\"bert len: {bert_len}, accu list: {seq_bert_len_sub_accu_list}\")\n",
    "                # try to separate sentences as evenly as possible\n",
    "                split_offset = np.argmin((np.array(seq_bert_len_sub_accu_list) - bert_len / 2) ** 2)\n",
    "                new_split_points.append(split_offset + split_points[idx] + 1)\n",
    "                # print(f\"split offset: {split_offset}\")\n",
    "        # print(f\"new split points: {new_split_points}\")\n",
    "\n",
    "        split_points += new_split_points\n",
    "        split_points.sort()\n",
    "\n",
    "        split_bert_lens = [sum(seq_bert_len_list[split_points[i]:split_points[i+1]]) for i in range(len(split_points)-1)]\n",
    "        # print(f\"split bert lengths: {split_bert_lens}\")\n",
    "\n",
    "    split_tks_seq_list = [tks_seq_list[split_points[i]:split_points[i+1]] for i in range(len(split_points)-1)]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7887\n",
      "bert len: 7887, accu list: [106, 186, 216, 267, 341, 415, 457, 496, 559, 597, 635, 663, 703, 722, 748, 786, 849, 891, 956, 1020, 1090, 1142, 1173, 1272, 1383, 1408, 1441, 1461, 1471, 1526, 1561, 1576, 1613, 1635, 1674, 1697, 1852, 1886, 1920, 1930, 1973, 2051, 2094, 2125, 2138, 2157, 2168, 2174, 2202, 2240, 2248, 2293, 2344, 2380, 2398, 2421, 2436, 2495, 2531, 2570, 2599, 2612, 2629, 2644, 2685, 2699, 2748, 2812, 2852, 2899, 2922, 2945, 2983, 2998, 3010, 3065, 3103, 3167, 3199, 3226, 3289, 3304, 3357, 3395, 3433, 3459, 3480, 3505, 3517, 3573, 3629, 3672, 3704, 3732, 3794, 3801, 3822, 3856, 3922, 4002, 4067, 4093, 4131, 4194, 4241, 4276, 4313, 4341, 4346, 4380, 4420, 4504, 4532, 4561, 4586, 4639, 4724, 4730, 4760, 4828, 4849, 4893, 4934, 4961, 5010, 5043, 5177, 5200, 5205, 5310, 5329, 5348, 5372, 5396, 5427, 5492, 5514, 5561, 5614, 5719, 5820, 5884, 5911, 5932, 5948, 5991, 6028, 6051, 6075, 6108, 6196, 6204, 6263, 6305, 6364, 6564, 6608, 6720, 6757, 6805, 6876, 6948, 6996, 7074, 7114, 7153, 7178, 7209, 7285, 7329, 7402, 7422, 7512, 7525, 7556, 7625, 7673, 7744, 7821, 7865, 7887]\n",
      "split offset: 98\n",
      "new split points: [99]\n",
      "split bert lengths: [3922, 3965]\n",
      "0 3922\n",
      "bert len: 3922, accu list: [106, 186, 216, 267, 341, 415, 457, 496, 559, 597, 635, 663, 703, 722, 748, 786, 849, 891, 956, 1020, 1090, 1142, 1173, 1272, 1383, 1408, 1441, 1461, 1471, 1526, 1561, 1576, 1613, 1635, 1674, 1697, 1852, 1886, 1920, 1930, 1973, 2051, 2094, 2125, 2138, 2157, 2168, 2174, 2202, 2240, 2248, 2293, 2344, 2380, 2398, 2421, 2436, 2495, 2531, 2570, 2599, 2612, 2629, 2644, 2685, 2699, 2748, 2812, 2852, 2899, 2922, 2945, 2983, 2998, 3010, 3065, 3103, 3167, 3199, 3226, 3289, 3304, 3357, 3395, 3433, 3459, 3480, 3505, 3517, 3573, 3629, 3672, 3704, 3732, 3794, 3801, 3822, 3856, 3922]\n",
      "split offset: 40\n",
      "1 3965\n",
      "bert len: 3965, accu list: [80, 145, 171, 209, 272, 319, 354, 391, 419, 424, 458, 498, 582, 610, 639, 664, 717, 802, 808, 838, 906, 927, 971, 1012, 1039, 1088, 1121, 1255, 1278, 1283, 1388, 1407, 1426, 1450, 1474, 1505, 1570, 1592, 1639, 1692, 1797, 1898, 1962, 1989, 2010, 2026, 2069, 2106, 2129, 2153, 2186, 2274, 2282, 2341, 2383, 2442, 2642, 2686, 2798, 2835, 2883, 2954, 3026, 3074, 3152, 3192, 3231, 3256, 3287, 3363, 3407, 3480, 3500, 3590, 3603, 3634, 3703, 3751, 3822, 3899, 3943, 3965]\n",
      "split offset: 43\n",
      "new split points: [41, 143]\n",
      "split bert lengths: [1973, 1949, 1989, 1976]\n",
      "0 1973\n",
      "bert len: 1973, accu list: [106, 186, 216, 267, 341, 415, 457, 496, 559, 597, 635, 663, 703, 722, 748, 786, 849, 891, 956, 1020, 1090, 1142, 1173, 1272, 1383, 1408, 1441, 1461, 1471, 1526, 1561, 1576, 1613, 1635, 1674, 1697, 1852, 1886, 1920, 1930, 1973]\n",
      "split offset: 18\n",
      "1 1949\n",
      "bert len: 1949, accu list: [78, 121, 152, 165, 184, 195, 201, 229, 267, 275, 320, 371, 407, 425, 448, 463, 522, 558, 597, 626, 639, 656, 671, 712, 726, 775, 839, 879, 926, 949, 972, 1010, 1025, 1037, 1092, 1130, 1194, 1226, 1253, 1316, 1331, 1384, 1422, 1460, 1486, 1507, 1532, 1544, 1600, 1656, 1699, 1731, 1759, 1821, 1828, 1849, 1883, 1949]\n",
      "split offset: 30\n",
      "2 1989\n",
      "bert len: 1989, accu list: [80, 145, 171, 209, 272, 319, 354, 391, 419, 424, 458, 498, 582, 610, 639, 664, 717, 802, 808, 838, 906, 927, 971, 1012, 1039, 1088, 1121, 1255, 1278, 1283, 1388, 1407, 1426, 1450, 1474, 1505, 1570, 1592, 1639, 1692, 1797, 1898, 1962, 1989]\n",
      "split offset: 23\n",
      "3 1976\n",
      "bert len: 1976, accu list: [21, 37, 80, 117, 140, 164, 197, 285, 293, 352, 394, 453, 653, 697, 809, 846, 894, 965, 1037, 1085, 1163, 1203, 1242, 1267, 1298, 1374, 1418, 1491, 1511, 1601, 1614, 1645, 1714, 1762, 1833, 1910, 1954, 1976]\n",
      "split offset: 17\n",
      "new split points: [19, 72, 123, 161]\n",
      "split bert lengths: [956, 1017, 972, 977, 1012, 977, 965, 1011]\n",
      "0 956\n",
      "bert len: 956, accu list: [106, 186, 216, 267, 341, 415, 457, 496, 559, 597, 635, 663, 703, 722, 748, 786, 849, 891, 956]\n",
      "split offset: 7\n",
      "1 1017\n",
      "bert len: 1017, accu list: [64, 134, 186, 217, 316, 427, 452, 485, 505, 515, 570, 605, 620, 657, 679, 718, 741, 896, 930, 964, 974, 1017]\n",
      "split offset: 8\n",
      "2 972\n",
      "bert len: 972, accu list: [78, 121, 152, 165, 184, 195, 201, 229, 267, 275, 320, 371, 407, 425, 448, 463, 522, 558, 597, 626, 639, 656, 671, 712, 726, 775, 839, 879, 926, 949, 972]\n",
      "split offset: 15\n",
      "3 977\n",
      "bert len: 977, accu list: [38, 53, 65, 120, 158, 222, 254, 281, 344, 359, 412, 450, 488, 514, 535, 560, 572, 628, 684, 727, 759, 787, 849, 856, 877, 911, 977]\n",
      "split offset: 12\n",
      "4 1012\n",
      "bert len: 1012, accu list: [80, 145, 171, 209, 272, 319, 354, 391, 419, 424, 458, 498, 582, 610, 639, 664, 717, 802, 808, 838, 906, 927, 971, 1012]\n",
      "split offset: 11\n",
      "5 977\n",
      "bert len: 977, accu list: [27, 76, 109, 243, 266, 271, 376, 395, 414, 438, 462, 493, 558, 580, 627, 680, 785, 886, 950, 977]\n",
      "split offset: 11\n",
      "6 965\n",
      "bert len: 965, accu list: [21, 37, 80, 117, 140, 164, 197, 285, 293, 352, 394, 453, 653, 697, 809, 846, 894, 965]\n",
      "split offset: 11\n",
      "7 1011\n",
      "bert len: 1011, accu list: [72, 120, 198, 238, 277, 302, 333, 409, 453, 526, 546, 636, 649, 680, 749, 797, 868, 945, 989, 1011]\n",
      "split offset: 9\n",
      "new split points: [8, 28, 57, 85, 111, 135, 155, 171]\n",
      "split bert lengths: [496, 460, 505, 512, 463, 509, 488, 489, 498, 514, 493, 484, 453, 512, 526, 485]\n",
      "0 496\n",
      "1 460\n",
      "2 505\n",
      "3 512\n",
      "bert len: 512, accu list: [10, 65, 100, 115, 152, 174, 213, 236, 391, 425, 459, 469, 512]\n",
      "split offset: 7\n",
      "4 463\n",
      "5 509\n",
      "6 488\n",
      "7 489\n",
      "8 498\n",
      "9 514\n",
      "bert len: 514, accu list: [84, 112, 141, 166, 219, 304, 310, 340, 408, 429, 473, 514]\n",
      "split offset: 4\n",
      "10 493\n",
      "11 484\n",
      "12 453\n",
      "13 512\n",
      "bert len: 512, accu list: [200, 244, 356, 393, 441, 512]\n",
      "split offset: 1\n",
      "14 526\n",
      "bert len: 526, accu list: [72, 120, 198, 238, 277, 302, 333, 409, 453, 526]\n",
      "split offset: 4\n",
      "15 485\n",
      "new split points: [36, 116, 157, 166]\n",
      "split bert lengths: [496, 460, 505, 236, 276, 463, 509, 488, 489, 498, 219, 295, 493, 484, 453, 244, 268, 277, 249, 485]\n"
     ]
    }
   ],
   "source": [
    "tks = data['8']['data']['text']\n",
    "tks_seq_list = [word_tokenize(sent) for sent in sent_tokenize(' '.join(tks))]\n",
    "seq_bert_len_list = [len(tokenizer.tokenize(' '.join(tks_seq), add_special_tokens=True)) for tks_seq in tks_seq_list]\n",
    "split_points = [0, len(tks_seq_list)]\n",
    "split_bert_lens = [sum(seq_bert_len_list[split_points[i]:split_points[i+1]]) for i in range(len(split_points)-1)]\n",
    "\n",
    "while (np.asarray(split_bert_lens) > max_seq_length).any():\n",
    "\n",
    "    new_split_points = list()\n",
    "    for idx, bert_len in enumerate(split_bert_lens):\n",
    "        print(idx, bert_len)\n",
    "        if bert_len > max_seq_length:\n",
    "            seq_bert_len_sub_list = seq_bert_len_list[split_points[idx]:split_points[idx+1]]\n",
    "            seq_bert_len_sub_accu_list = list(itertools.accumulate(seq_bert_len_sub_list, operator.add))\n",
    "            print(f\"bert len: {bert_len}, accu list: {seq_bert_len_sub_accu_list}\")\n",
    "            # try to separate sentences as evenly as possible\n",
    "            split_offset = np.argmin((np.array(seq_bert_len_sub_accu_list) - bert_len / 2) ** 2)\n",
    "            new_split_points.append(split_offset + split_points[idx] + 1)\n",
    "            print(f\"split offset: {split_offset}\")\n",
    "    print(f\"new split points: {new_split_points}\")\n",
    "\n",
    "    split_points += new_split_points\n",
    "    split_points.sort()\n",
    "\n",
    "    split_bert_lens = [sum(seq_bert_len_list[split_points[i]:split_points[i+1]]) for i in range(len(split_points)-1)]\n",
    "    print(f\"split bert lengths: {split_bert_lens}\")\n",
    "\n",
    "split_tks_seq_list = [tks_seq_list[split_points[i]:split_points[i+1]] for i in range(len(split_points)-1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from chemdataextractor.nlp.tokenize import ChemWordTokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cwt = ChemWordTokenizer()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cwt.tokenize(' '.join(ori_tks))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from chemdataextractor.doc import Paragraph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "para = Paragraph(' '.join(ori_tks))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "para.raw_sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tks = para.tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(tks[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(merge_list_of_lists(tks))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(merge_list_of_lists([sent.tokens for sent in para.sentences]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "para.sentences[0].end"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
